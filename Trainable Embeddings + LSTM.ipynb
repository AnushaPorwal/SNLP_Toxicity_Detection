{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fbebcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import LSTM, Activation, Dropout, Dense, Input\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Model\n",
    "import string\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03128849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "631d3f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c000cbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91bcea5",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5fa1f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train_2024.csv', quoting=3)\n",
    "eval_data = pd.read_csv('data/test_2024.csv', quoting=3)\n",
    "dev_data = pd.read_csv('data/dev_2024.csv', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c057e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(data):\n",
    "    data['text_wo_stopwords'] = data['text'].apply(lambda x : ' '.join([word for word in x.split() if word not in stopwords]))\n",
    "    return data\n",
    "\n",
    "def remove_tags(test_str):\n",
    "    punc = '''()-[]{};:'\"\\,<>/@#$%^&*_~'''\n",
    "    for ele in test_str:\n",
    "        if ele in punc:\n",
    "            test_str = test_str.replace(ele, \"\")\n",
    "    return test_str\n",
    "\n",
    "def split_join(data):\n",
    "    data['cleaner_text'] = data['clean_text'].apply(lambda x : ' '.join([word for word in re.findall( r'\\w+|[^\\s\\w]+', x)]))\n",
    "    return data\n",
    "\n",
    "def get_X_Y(data):\n",
    "    '''\n",
    "        Takes in df data\n",
    "        Processes the text in the dataframe\n",
    "        returns the texts as a list, and labels as an array\n",
    "    '''\n",
    "    data = remove_stopwords(data)\n",
    "    data['clean_text']= data['text_wo_stopwords'].apply(lambda cw : remove_tags(cw))\n",
    "    data = split_join(data)\n",
    "    \n",
    "    text_list = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        text_list.append(data['cleaner_text'][i])\n",
    "\n",
    "    labels = np.array(data['label'])\n",
    "    return text_list, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ab80db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = get_X_Y(train_data)\n",
    "X_eval, Y_eval= get_X_Y(eval_data)\n",
    "X_dev, Y_dev = get_X_Y(dev_data)\n",
    "\n",
    "X_train.extend(X_dev)\n",
    "X = X_train\n",
    "Y = np.concatenate((Y_train, Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bed363f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5504cbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88000, 22000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xtrain), len(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bfc3c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data_ids = np.array(eval_data['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86d9ddb",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70a54aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100911\n"
     ]
    }
   ],
   "source": [
    "#Find how many unique tokens are in the train set:\n",
    "vocab = set()\n",
    "for i in xtrain:\n",
    "    words = i.split()\n",
    "    vocab.update(words)\n",
    "    \n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b1d3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_tok = '<OOV>'\n",
    "tokenizer = Tokenizer(num_words = vocab_size, char_level = False, oov_token = oov_tok)\n",
    "tokenizer.fit_on_texts(xtrain)\n",
    "\n",
    "words_to_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99b3d180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80426\n"
     ]
    }
   ],
   "source": [
    "total_words = len(words_to_index)\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df3a6cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen=50\n",
    "training_sequences = tokenizer.texts_to_sequences(xtrain)\n",
    "training_padded = pad_sequences(training_sequences, maxlen = maxLen, padding = 'post', truncating = 'post')\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(xtest)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen = maxLen, padding = 'post', truncating = 'post')\n",
    "\n",
    "eval_sequences = tokenizer.texts_to_sequences(X_eval)\n",
    "eval_padded = pad_sequences(eval_sequences, maxlen = maxLen, padding = 'post', truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92381682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training tensor:  (88000, 50)\n",
      "Shape of testing tensor:  (22000, 50)\n",
      "Shape of dev tensor:  (12001, 50)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of training tensor: ', training_padded.shape)\n",
    "print('Shape of testing tensor: ', testing_padded.shape)\n",
    "print('Shape of dev tensor: ', eval_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260a7d94",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2d2d01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "def toxicity_model(input_shape):\n",
    "    X_indices = Input(input_shape)\n",
    "    X = Embedding(vocab_size, embedding_dim, input_length=maxLen)(X_indices)\n",
    "    X = LSTM(128, return_sequences=True)(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = LSTM(128, return_sequences=True)(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = LSTM(128)(X)\n",
    "    X = Dense(1, activation='sigmoid')(X)\n",
    "    \n",
    "    model = Model(inputs=X_indices, outputs=X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a35d52fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = toxicity_model(maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6ef3a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 50, 16)            1614576   \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 50, 128)           74240     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 50, 128)           0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 50, 128)           131584    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 50, 128)           0         \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1952113 (7.45 MB)\n",
      "Trainable params: 1952113 (7.45 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7874924a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1375/1375 [==============================] - 788s 563ms/step - loss: 0.3158 - accuracy: 0.8645\n",
      "Epoch 2/5\n",
      "1375/1375 [==============================] - 740s 538ms/step - loss: 0.1698 - accuracy: 0.9392\n",
      "Epoch 3/5\n",
      "1375/1375 [==============================] - 748s 544ms/step - loss: 0.1310 - accuracy: 0.9538\n",
      "Epoch 4/5\n",
      "1375/1375 [==============================] - 790s 575ms/step - loss: 0.1040 - accuracy: 0.9631\n",
      "Epoch 5/5\n",
      "1375/1375 [==============================] - 763s 555ms/step - loss: 0.0824 - accuracy: 0.9705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23c8911e090>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = keras.optimizers.Adam(learning_rate = 0.001)\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_padded, ytrain, batch_size=64, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d61cda9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/trainable_embeds_lstm_5.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33348934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "688/688 [==============================] - 73s 98ms/step\n"
     ]
    }
   ],
   "source": [
    "test_preds = model.predict(testing_padded)\n",
    "\n",
    "test_pred_labels = []\n",
    "for i in range(len(test_preds)):\n",
    "    if test_preds[i] > 0.5:\n",
    "        test_pred_labels.append(1)\n",
    "    else:\n",
    "        test_pred_labels.append(0)\n",
    "        \n",
    "test_pred_labels = np.array(test_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d7a239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9085454545454545 0.8756796836381611\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "acc = accuracy_score(ytest, test_pred_labels)\n",
    "f1 = f1_score(ytest, test_pred_labels)\n",
    "print(acc, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37f694a",
   "metadata": {},
   "source": [
    "Evaluating on the Eval Set (hidden test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c3f767b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376/376 [==============================] - 35s 93ms/step\n"
     ]
    }
   ],
   "source": [
    "eval_preds = model.predict(eval_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8e5762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pred_labels = []\n",
    "for i in range(len(eval_preds)):\n",
    "    if eval_preds[i] > 0.5:\n",
    "        eval_pred_labels.append(1)\n",
    "    else:\n",
    "        eval_pred_labels.append(0)\n",
    "        \n",
    "eval_pred_labels = np.array(eval_pred_labels)\n",
    "\n",
    "preds_df = pd.DataFrame({'label': eval_pred_labels})\n",
    "\n",
    "eval_data_ids = eval_data['id']\n",
    "dev_set_ids = pd.DataFrame({'id': eval_data_ids})\n",
    "\n",
    "final_output = pd.concat([dev_set_ids, preds_df], axis=1)\n",
    "final_output.to_csv('output2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fdb71c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output.to_csv('TrainableEmbeddings_LSTM.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c536ac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fei_op = pd.read_csv('submission-stack.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3dee5baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_output) == len(fei_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cfabc972",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matches = 0\n",
    "for i in range(0, len(fei_op)):\n",
    "    if final_output['label'][i] == fei_op['label'][i]:\n",
    "        count_matches += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "486e2ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9143404716273644"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_matches/len(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae900655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10973"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38857de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
